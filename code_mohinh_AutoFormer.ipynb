{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNUiPqobGG_7",
        "outputId": "f1acf25b-867c-4ab5-a0ce-b22001b25809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  tatrach-autoformer-main.zip\n",
            "   creating: tatrach-autoformer-main/\n",
            "  inflating: tatrach-autoformer-main/.gitignore  \n",
            "   creating: tatrach-autoformer-main/dataset/\n",
            "  inflating: tatrach-autoformer-main/dataset/Data_TaTrach.csv  \n",
            "   creating: tatrach-autoformer-main/dataset/ETT-small/\n",
            "  inflating: tatrach-autoformer-main/dataset/ETT-small/ETTh1.csv  \n",
            "   creating: tatrach-autoformer-main/data_provider/\n",
            "  inflating: tatrach-autoformer-main/data_provider/data_factory.py  \n",
            "  inflating: tatrach-autoformer-main/data_provider/data_loader.py  \n",
            " extracting: tatrach-autoformer-main/data_provider/__init__.py  \n",
            "  inflating: tatrach-autoformer-main/Dockerfile  \n",
            "  inflating: tatrach-autoformer-main/environment.yml  \n",
            "   creating: tatrach-autoformer-main/exp/\n",
            "  inflating: tatrach-autoformer-main/exp/exp_basic.py  \n",
            "  inflating: tatrach-autoformer-main/exp/exp_main.py  \n",
            " extracting: tatrach-autoformer-main/exp/__init__.py  \n",
            "   creating: tatrach-autoformer-main/layers/\n",
            "  inflating: tatrach-autoformer-main/layers/AutoCorrelation.py  \n",
            "  inflating: tatrach-autoformer-main/layers/Autoformer_EncDec.py  \n",
            "  inflating: tatrach-autoformer-main/layers/Embed.py  \n",
            "  inflating: tatrach-autoformer-main/layers/SelfAttention_Family.py  \n",
            "  inflating: tatrach-autoformer-main/layers/Transformer_EncDec.py  \n",
            " extracting: tatrach-autoformer-main/layers/__init__.py  \n",
            "  inflating: tatrach-autoformer-main/LICENSE  \n",
            "  inflating: tatrach-autoformer-main/Makefile  \n",
            "   creating: tatrach-autoformer-main/models/\n",
            "  inflating: tatrach-autoformer-main/models/Autoformer.py  \n",
            "  inflating: tatrach-autoformer-main/models/Informer.py  \n",
            "  inflating: tatrach-autoformer-main/models/Reformer.py  \n",
            "  inflating: tatrach-autoformer-main/models/Transformer.py  \n",
            " extracting: tatrach-autoformer-main/models/__init__.py  \n",
            "   creating: tatrach-autoformer-main/pic/\n",
            "  inflating: tatrach-autoformer-main/pic/Auto-Correlation.png  \n",
            "  inflating: tatrach-autoformer-main/pic/Autoformer.png  \n",
            "  inflating: tatrach-autoformer-main/pic/results.png  \n",
            "  inflating: tatrach-autoformer-main/predict.ipynb  \n",
            "  inflating: tatrach-autoformer-main/README.md  \n",
            "  inflating: tatrach-autoformer-main/requirements.txt  \n",
            "  inflating: tatrach-autoformer-main/run.py  \n",
            "   creating: tatrach-autoformer-main/scripts/\n",
            "   creating: tatrach-autoformer-main/scripts/ECL_script/\n",
            "  inflating: tatrach-autoformer-main/scripts/ECL_script/Autoformer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ECL_script/Informer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ECL_script/Reformer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ECL_script/Transformer.sh  \n",
            "   creating: tatrach-autoformer-main/scripts/ETT_script/\n",
            "  inflating: tatrach-autoformer-main/scripts/ETT_script/Autoformer_ETTh1.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ETT_script/Autoformer_ETTh2.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ETT_script/Autoformer_ETTm1.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ETT_script/Autoformer_ETTm2.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ETT_script/Autoformer_univariate.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ETT_script/Informer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ETT_script/Reformer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ETT_script/Transformer.sh  \n",
            "   creating: tatrach-autoformer-main/scripts/Exchange_script/\n",
            "  inflating: tatrach-autoformer-main/scripts/Exchange_script/Autoformer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/Exchange_script/Autoformer_univariate.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/Exchange_script/Informer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/Exchange_script/Reformer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/Exchange_script/Transformer.sh  \n",
            "   creating: tatrach-autoformer-main/scripts/ILI_script/\n",
            "  inflating: tatrach-autoformer-main/scripts/ILI_script/Autoformer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ILI_script/Informer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ILI_script/Reformer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/ILI_script/Transformer.sh  \n",
            "   creating: tatrach-autoformer-main/scripts/Traffic_script/\n",
            "  inflating: tatrach-autoformer-main/scripts/Traffic_script/Autoformer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/Traffic_script/Informer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/Traffic_script/Reformer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/Traffic_script/Transformer.sh  \n",
            "   creating: tatrach-autoformer-main/scripts/Weather_script/\n",
            "  inflating: tatrach-autoformer-main/scripts/Weather_script/Autoformer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/Weather_script/Informer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/Weather_script/Reformer.sh  \n",
            "  inflating: tatrach-autoformer-main/scripts/Weather_script/Transformer.sh  \n",
            "  inflating: tatrach-autoformer-main/tatrach.ipynb  \n",
            " extracting: tatrach-autoformer-main/tatrach.rar  \n",
            "   creating: tatrach-autoformer-main/utils/\n",
            "  inflating: tatrach-autoformer-main/utils/download_data.py  \n",
            "  inflating: tatrach-autoformer-main/utils/masking.py  \n",
            "  inflating: tatrach-autoformer-main/utils/metrics.py  \n",
            "  inflating: tatrach-autoformer-main/utils/timefeatures.py  \n",
            "  inflating: tatrach-autoformer-main/utils/tools.py  \n",
            " extracting: tatrach-autoformer-main/utils/__init__.py  \n",
            "  inflating: tatrach-autoformer-main/dataset/mucnuoc_gio_preprocess.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip tatrach-autoformer-main.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\t\t # æŸ¥çœ‹GPuè®¾å¤‡æ˜¯å¦å¯ç”¨\n",
        "print(torch.cuda.device_count()) \t\t# æŸ¥çœ‹GPuè®¾å¤‡æ•°é‡\n",
        "print(torch.cuda.get_device_name())   \t# æŸ¥çœ‹å½“å‰GPuè®¾å¤‡åç§°ï¼Œé»˜è®¤è®¾å¤‡idä»Ž0å¼€å§‹\n",
        "print(torch.cuda.current_device())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCKb5vdtGI6z",
        "outputId": "5910f5c1-2558-4ae1-afb4-539ed643f248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "1\n",
            "NVIDIA L4\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reformer-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7fOxl32GKUr",
        "outputId": "d94cba94-397c-4b9d-c8eb-fe3541d43981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reformer-pytorch\n",
            "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting axial-positional-embedding>=0.1.0 (from reformer-pytorch)\n",
            "  Downloading axial_positional_embedding-0.3.12-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from reformer-pytorch) (0.8.1)\n",
            "Collecting local-attention (from reformer-pytorch)\n",
            "  Downloading local_attention-1.11.1-py3-none-any.whl.metadata (907 bytes)\n",
            "Collecting product-key-memory (from reformer-pytorch)\n",
            "  Downloading product_key_memory-0.2.11-py3-none-any.whl.metadata (717 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from reformer-pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->reformer-pytorch) (1.3.0)\n",
            "Collecting hyper-connections>=0.1.8 (from local-attention->reformer-pytorch)\n",
            "  Downloading hyper_connections-0.2.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting colt5-attention>=0.10.14 (from product-key-memory->reformer-pytorch)\n",
            "  Downloading CoLT5_attention-0.11.1-py3-none-any.whl.metadata (737 bytes)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from colt5-attention>=0.10.14->product-key-memory->reformer-pytorch) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->reformer-pytorch) (3.0.2)\n",
            "Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
            "Downloading axial_positional_embedding-0.3.12-py3-none-any.whl (6.7 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading local_attention-1.11.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading product_key_memory-0.2.11-py3-none-any.whl (6.5 kB)\n",
            "Downloading CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\n",
            "Downloading hyper_connections-0.2.1-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, hyper-connections, axial-positional-embedding, local-attention, colt5-attention, product-key-memory, reformer-pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed axial-positional-embedding-0.3.12 colt5-attention-0.11.1 hyper-connections-0.2.1 local-attention-1.11.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 product-key-memory-0.2.11 reformer-pytorch-1.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "from exp.exp_main import Exp_Main#exp stands for experiments\n",
        "import random\n",
        "import numpy as np\n",
        "from utils.tools import dotdict\n",
        "\n",
        "# fix_seed = 2021\n",
        "# random.seed(fix_seed)\n",
        "# torch.manual_seed(fix_seed)\n",
        "# np.random.seed(fix_seed)\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
        "\n",
        "# basic config\n",
        "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
        "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')#æ¨¡åž‹id\n",
        "parser.add_argument('--model', type=str, required=True, default='Autoformer',#é€‰æ‹©æ¨¡åž‹\n",
        "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
        "\n",
        "# data loader\n",
        "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')#æ•°æ®ç±»åž‹\n",
        "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')#æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„\n",
        "parser.add_argument('--data_path', type=str, default='mucnuoc_gio_preprocess.csv', help='data file')#å…·ä½“æ–‡ä»¶\n",
        "parser.add_argument('--features', type=str, default='M',\n",
        "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')#é¢„æµ‹ç±»åˆ«\n",
        "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')#ä¸å¤ªæ‡‚ OTå¥½åƒä»£è¡¨Output Target,è¦é¢„æµ‹çš„å•å˜é‡\n",
        "parser.add_argument('--freq', type=str, default='h',\n",
        "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
        "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')#ä¿å­˜æ¨¡åž‹\n",
        "\n",
        "# forecasting task\n",
        "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')#è¾“å…¥åºåˆ—é•¿åº¦\n",
        "parser.add_argument('--label_len', type=int, default=48, help='start token length')#è¿™ä¸ªlabel_lenæœªå®Œå…¨æžæ‡‚\n",
        "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')#è¾“å‡ºåºåˆ—é•¿åº¦\n",
        "\n",
        "# model define\n",
        "parser.add_argument('--bucket_size', type=int, default=4, help='for Reformer')#Reformerä¸“ç”¨å±žæ€§\n",
        "parser.add_argument('--n_hashes', type=int, default=4, help='for Reformer')#Reformerä¸“ç”¨å±žæ€§\n",
        "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')#encoder input size\n",
        "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')#decoder input size\n",
        "parser.add_argument('--c_out', type=int, default=7, help='output size')#è¾“å‡ºé•¿åº¦\n",
        "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')#dimension of model\n",
        "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')#num of heads\n",
        "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')#num of encoder layers\n",
        "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')#num of decoder layers\n",
        "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')#dimension of fcn\n",
        "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')#çª—å£æ»‘åŠ¨å¹³å‡æ•°\n",
        "parser.add_argument('--factor', type=int, default=1, help='attn factor')#attn factorä¸å¤ªç†è§£\n",
        "parser.add_argument('--distil', action='store_false',\n",
        "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
        "                    default=True)#æ˜¯å¦åœ¨encoderé‡Œé¢ä½¿ç”¨çŸ¥è¯†è’¸é¦\n",
        "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')#dropout\n",
        "parser.add_argument('--embed', type=str, default='timeF',\n",
        "                    help='time features encoding, options:[timeF, fixed, learned]')#time features encodingä¸å¤ªèƒ½getåˆ°\n",
        "parser.add_argument('--activation', type=str, default='gelu', help='activation')#æ¿€æ´»å‡½æ•°default=gelu\n",
        "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in encoder')#encoderçš„output_attentionæ˜¯å¦è¾“å‡º\n",
        "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')#æ˜¯å¦é¢„æµ‹æœªè§çš„æœªæ¥æ•°æ®,ä¹Ÿå°±æ˜¯æ˜¯å¦è¿›è¡ŒæŽ¨ç†çš„æ„æ€\n",
        "\n",
        "# optimization\n",
        "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')# num_workersæ˜¯åŠ è½½æ•°æ®(batch)çš„çº¿ç¨‹æ•°ç›®\n",
        "parser.add_argument('--itr', type=int, default=2, help='experiments times')#å®žéªŒæ¬¡æ•°\n",
        "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')#å°±æ˜¯epoch\n",
        "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')#bathsize\n",
        "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')#patience: å½“early stopè¢«æ¿€æ´»(å¦‚å‘çŽ°lossç›¸æ¯”ä¸Šä¸€ä¸ªepochè®­ç»ƒæ²¡æœ‰ä¸‹é™)ï¼Œåˆ™ç»è¿‡patienceä¸ªepochåŽåœæ­¢è®­ç»ƒ\n",
        "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')#lr\n",
        "parser.add_argument('--des', type=str, default='test', help='exp description')#test\n",
        "parser.add_argument('--loss', type=str, default='mse', help='loss function')#loss is mse\n",
        "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')#adjust learning-rate\n",
        "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)#ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ\n",
        "\n",
        "# GPU\n",
        "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
        "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
        "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
        "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args = dotdict()\n",
        "args.target = 'OT'\n",
        "args.des = 'test'\n",
        "args.dropout = 0.05\n",
        "args.num_workers = 10\n",
        "args.gpu = 0\n",
        "args.lradj = 'type1'\n",
        "args.devices = '0'\n",
        "args.use_gpu = True\n",
        "args.use_multi_gpu = False\n",
        "# if args.use_gpu and args.use_multi_gpu: #æ˜¯å¦ä½¿ç”¨å¤šå¡çš„åˆ¤æ–­\n",
        "#     args.dvices = args.devices.replace(' ', '')\n",
        "#     device_ids = args.devices.split(',')\n",
        "#     args.device_ids = [int(id_) for id_ in device_ids]\n",
        "#     args.gpu = args.device_ids[0]\n",
        "args.freq = 'h'\n",
        "args.checkpoints = './checkpoints/'\n",
        "args.bucket_size = 4\n",
        "args.n_hashes = 4\n",
        "args.seq_len = 24\n",
        "args.label_len = 1\n",
        "args.pred_len = 72\n",
        "args.e_layers = 6\n",
        "args.d_layers = 5\n",
        "args.n_heads = 32\n",
        "args.factor = 1\n",
        "args.d_model = 512\n",
        "args.des = 'Exp'\n",
        "args.itr = 1\n",
        "args.d_ff = 2048\n",
        "args.moving_avg = 25\n",
        "args.distil = True\n",
        "args.output_attention = False\n",
        "args.patience= 3\n",
        "args.learning_rate = 1e-4\n",
        "args.batch_size = 32\n",
        "args.embed = 'timeF'\n",
        "args.activation = 'gelu'\n",
        "args.use_amp = False\n",
        "args.loss = 'mse'\n",
        "\n",
        "args.train_epochs = 10\n",
        "args.is_training = True\n",
        "args.enc_in = 1\n",
        "args.dec_in = 1\n",
        "args.c_out = 1\n",
        "args.target = \"q64\"\n",
        "args.root_path = './dataset/'\n",
        "args.data_path ='mucnuoc_gio_preprocess.csv'\n",
        "args.model_id='q64'\n",
        "args.model = 'Autoformer'\n",
        "#args.model = 'Informer'\n",
        "args.data = 'custom'\n",
        "args.features = 'S'\n",
        "\n",
        "print('Args in experiment:')\n",
        "print(args)\n",
        "\n",
        "\n",
        "\n",
        "Exp = Exp_Main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjMa6vcmGLsH",
        "outputId": "019f9c8a-a0cd-41d0-d9b5-b569f71722f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Args in experiment:\n",
            "{'target': 'q64', 'des': 'Exp', 'dropout': 0.05, 'num_workers': 10, 'gpu': 0, 'lradj': 'type1', 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False, 'freq': 'h', 'checkpoints': './checkpoints/', 'bucket_size': 4, 'n_hashes': 4, 'seq_len': 24, 'label_len': 1, 'pred_len': 72, 'e_layers': 6, 'd_layers': 5, 'n_heads': 32, 'factor': 1, 'd_model': 512, 'itr': 1, 'd_ff': 2048, 'moving_avg': 25, 'distil': True, 'output_attention': False, 'patience': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'embed': 'timeF', 'activation': 'gelu', 'use_amp': False, 'loss': 'mse', 'train_epochs': 10, 'is_training': True, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'root_path': './dataset/', 'data_path': 'mucnuoc_gio_preprocess.csv', 'model_id': 'q64', 'model': 'Autoformer', 'data': 'custom', 'features': 'S'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "def save_model_simple(exp, args, save_dir=\"/content/save_model\"):\n",
        "    \"\"\"LÆ°u model Ä‘Æ¡n giáº£n: H5 + PKL files\"\"\"\n",
        "\n",
        "    # Táº¡o folder\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    print(f\"ðŸ’¾ Saving model to: {save_dir}\")\n",
        "\n",
        "    # 1. LÆ°u model weights (.h5)\n",
        "    model_path = os.path.join(save_dir, f\"autoformer_model_window_{args.target}.h5\")\n",
        "    with h5py.File(model_path, 'w') as f:\n",
        "        # LÆ°u weights\n",
        "        for key, tensor in exp.model.state_dict().items():\n",
        "            if isinstance(tensor, torch.Tensor):\n",
        "                f.create_dataset(key, data=tensor.detach().cpu().numpy())\n",
        "\n",
        "    print(f\"âœ… Model saved: {os.path.basename(model_path)}\")\n",
        "\n",
        "    # 2. LÆ°u scaler (.pkl)\n",
        "    print(\"Debugging scaler save...\")\n",
        "    scaler_saved = False\n",
        "\n",
        "    # Thá»­ cÃ¡ch 2: Láº¥y tá»« data loader khÃ¡c\n",
        "    if not scaler_saved:\n",
        "        try:\n",
        "            print(\"   Trying method 2: from data_loader directly...\")\n",
        "            train_data, train_loader = exp._get_data(flag='train')\n",
        "\n",
        "            # Kiá»ƒm tra cÃ¡c thuá»™c tÃ­nh cÃ³ thá»ƒ chá»©a scaler\n",
        "            for attr_name in ['scaler', 'data_scaler', 'transform', 'normalizer']:\n",
        "                if hasattr(train_data, attr_name):\n",
        "                    scaler = getattr(train_data, attr_name)\n",
        "                    print(f\"   Found scaler in attribute: {attr_name}\")\n",
        "\n",
        "                    scaler_path = os.path.join(save_dir, f\"scaler_{args.target}_{timestamp}.pkl\")\n",
        "                    with open(scaler_path, 'wb') as f:\n",
        "                        pickle.dump(scaler, f)\n",
        "                    print(f\"âœ… Scaler saved: {os.path.basename(scaler_path)}\")\n",
        "                    scaler_saved = True\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Method 2 failed: {e}\")\n",
        "\n",
        "    # 3. LÆ°u config (.pkl)\n",
        "    config = {\n",
        "        'model': args.model,\n",
        "        'target': args.target,\n",
        "        'seq_len': args.seq_len,\n",
        "        'pred_len': args.pred_len,\n",
        "        'd_model': args.d_model,\n",
        "        'n_heads': args.n_heads,\n",
        "        'e_layers': args.e_layers,\n",
        "        'd_layers': args.d_layers,\n",
        "        'enc_in': args.enc_in,\n",
        "        'dec_in': args.dec_in,\n",
        "        'c_out': args.c_out,\n",
        "        'features': args.features,\n",
        "        'dropout': args.dropout,\n",
        "        'factor': args.factor,\n",
        "        'moving_avg': getattr(args, 'moving_avg', 25),\n",
        "        'activation': args.activation,\n",
        "        'embed': args.embed,\n",
        "        'freq': args.freq\n",
        "    }\n",
        "    print(args.pred_len)\n",
        "    config_path = os.path.join(save_dir, f\"config_autoformer_window_{args.target}.pkl\")\n",
        "    with open(config_path, 'wb') as f:\n",
        "        pickle.dump(config, f)\n",
        "\n",
        "    print(f\"âœ… Config saved: {os.path.basename(config_path)}\")\n",
        "    print(f\"ðŸ“ All files saved in: {save_dir}\")\n",
        "\n",
        "    return save_dir"
      ],
      "metadata": {
        "id": "GWE5K9T7YTuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.is_training:\n",
        "  for ii in range(args.itr):\n",
        "      # setting record of experiments\n",
        "      setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
        "          args.model_id,\n",
        "          args.model,\n",
        "          args.data,\n",
        "          args.features,\n",
        "          args.seq_len,\n",
        "          args.label_len,\n",
        "          args.pred_len,\n",
        "          args.d_model,\n",
        "          args.n_heads,\n",
        "          args.e_layers,\n",
        "          args.d_layers,\n",
        "          args.d_ff,\n",
        "          args.factor,\n",
        "          args.embed,\n",
        "          args.distil,\n",
        "          args.des, ii)\n",
        "\n",
        "      exp = Exp(args)  # set experiments\n",
        "      print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
        "      exp.train(setting)\n",
        "\n",
        "      print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "      exp.test(setting)\n",
        "\n",
        "      if args.do_predict:\n",
        "          print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "          exp.predict(setting, True)\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "else:\n",
        "  ii = 0\n",
        "  setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(args.model_id,\n",
        "                                                                                                args.model,\n",
        "                                                                                                args.data,\n",
        "                                                                                                args.features,\n",
        "                                                                                                args.seq_len,\n",
        "                                                                                                args.label_len,\n",
        "                                                                                                args.pred_len,\n",
        "                                                                                                args.d_model,\n",
        "                                                                                                args.n_heads,\n",
        "                                                                                                args.e_layers,\n",
        "                                                                                                args.d_layers,\n",
        "                                                                                                args.d_ff,\n",
        "                                                                                                args.factor,\n",
        "                                                                                                args.embed,\n",
        "                                                                                                args.distil,\n",
        "                                                                                                args.des, ii)\n",
        "\n",
        "  exp = Exp(args)  # set experiments\n",
        "  print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "  exp.test(setting, test=1)\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKu2Ptv9GNQn",
        "outputId": "4b955f82-5acb-4cb6-e13e-6b049fc16b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : q64_Autoformer_custom_ftS_sl24_ll1_pl72_dm512_nh32_el6_dl5_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 33157\n",
            "val 4680\n",
            "test 9429\n",
            "\titers: 100, epoch: 1 | loss: 0.0365545\n",
            "\tspeed: 0.1197s/iter; left time: 1228.5124s\n",
            "\titers: 200, epoch: 1 | loss: 0.0137743\n",
            "\tspeed: 0.1173s/iter; left time: 1191.6078s\n",
            "\titers: 300, epoch: 1 | loss: 0.0128247\n",
            "\tspeed: 0.1166s/iter; left time: 1173.5936s\n",
            "\titers: 400, epoch: 1 | loss: 0.0107844\n",
            "\tspeed: 0.1157s/iter; left time: 1152.6943s\n",
            "\titers: 500, epoch: 1 | loss: 0.0111502\n",
            "\tspeed: 0.1151s/iter; left time: 1134.9631s\n",
            "\titers: 600, epoch: 1 | loss: 0.0108958\n",
            "\tspeed: 0.1148s/iter; left time: 1120.1758s\n",
            "\titers: 700, epoch: 1 | loss: 0.0135873\n",
            "\tspeed: 0.1149s/iter; left time: 1110.4087s\n",
            "\titers: 800, epoch: 1 | loss: 0.0093423\n",
            "\tspeed: 0.1151s/iter; left time: 1100.6406s\n",
            "\titers: 900, epoch: 1 | loss: 0.0141152\n",
            "\tspeed: 0.1156s/iter; left time: 1093.2691s\n",
            "\titers: 1000, epoch: 1 | loss: 0.0126289\n",
            "\tspeed: 0.1157s/iter; left time: 1083.0069s\n",
            "Epoch: 1 cost time: 120.30702662467957\n",
            "Epoch: 1, Steps: 1036 | Train Loss: 0.0334065 Vali Loss: 0.0287543 Test Loss: 0.0257402\n",
            "Validation loss decreased (inf --> 0.028754).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.0086613\n",
            "\tspeed: 0.4609s/iter; left time: 4252.0700s\n",
            "\titers: 200, epoch: 2 | loss: 0.0082965\n",
            "\tspeed: 0.1153s/iter; left time: 1052.4381s\n",
            "\titers: 300, epoch: 2 | loss: 0.0111106\n",
            "\tspeed: 0.1157s/iter; left time: 1043.8877s\n",
            "\titers: 400, epoch: 2 | loss: 0.0081373\n",
            "\tspeed: 0.1152s/iter; left time: 1028.2302s\n",
            "\titers: 500, epoch: 2 | loss: 0.0074623\n",
            "\tspeed: 0.1152s/iter; left time: 1016.8666s\n",
            "\titers: 600, epoch: 2 | loss: 0.0071594\n",
            "\tspeed: 0.1152s/iter; left time: 1005.3193s\n",
            "\titers: 700, epoch: 2 | loss: 0.0055257\n",
            "\tspeed: 0.1150s/iter; left time: 991.7946s\n",
            "\titers: 800, epoch: 2 | loss: 0.0056199\n",
            "\tspeed: 0.1154s/iter; left time: 983.8835s\n",
            "\titers: 900, epoch: 2 | loss: 0.0067988\n",
            "\tspeed: 0.1157s/iter; left time: 974.3838s\n",
            "\titers: 1000, epoch: 2 | loss: 0.0056070\n",
            "\tspeed: 0.1153s/iter; left time: 960.0852s\n",
            "Epoch: 2 cost time: 119.83583474159241\n",
            "Epoch: 2, Steps: 1036 | Train Loss: 0.0082015 Vali Loss: 0.0254483 Test Loss: 0.0239785\n",
            "Validation loss decreased (0.028754 --> 0.025448).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.0050750\n",
            "\tspeed: 0.4622s/iter; left time: 3784.6298s\n",
            "\titers: 200, epoch: 3 | loss: 0.0044168\n",
            "\tspeed: 0.1154s/iter; left time: 933.3568s\n",
            "\titers: 300, epoch: 3 | loss: 0.0047867\n",
            "\tspeed: 0.1153s/iter; left time: 921.2475s\n",
            "\titers: 400, epoch: 3 | loss: 0.0046947\n",
            "\tspeed: 0.1153s/iter; left time: 909.5491s\n",
            "\titers: 500, epoch: 3 | loss: 0.0082816\n",
            "\tspeed: 0.1152s/iter; left time: 896.9428s\n",
            "\titers: 600, epoch: 3 | loss: 0.0055729\n",
            "\tspeed: 0.1150s/iter; left time: 884.0333s\n",
            "\titers: 700, epoch: 3 | loss: 0.0054024\n",
            "\tspeed: 0.1148s/iter; left time: 870.8818s\n",
            "\titers: 800, epoch: 3 | loss: 0.0103524\n",
            "\tspeed: 0.1150s/iter; left time: 861.1278s\n",
            "\titers: 900, epoch: 3 | loss: 0.0102421\n",
            "\tspeed: 0.1151s/iter; left time: 850.5406s\n",
            "\titers: 1000, epoch: 3 | loss: 0.0040728\n",
            "\tspeed: 0.1151s/iter; left time: 838.6114s\n",
            "Epoch: 3 cost time: 119.63206720352173\n",
            "Epoch: 3, Steps: 1036 | Train Loss: 0.0068037 Vali Loss: 0.0238982 Test Loss: 0.0222664\n",
            "Validation loss decreased (0.025448 --> 0.023898).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.0083977\n",
            "\tspeed: 0.4624s/iter; left time: 3307.5046s\n",
            "\titers: 200, epoch: 4 | loss: 0.0056508\n",
            "\tspeed: 0.1154s/iter; left time: 813.7962s\n",
            "\titers: 300, epoch: 4 | loss: 0.0048129\n",
            "\tspeed: 0.1153s/iter; left time: 801.7166s\n",
            "\titers: 400, epoch: 4 | loss: 0.0075033\n",
            "\tspeed: 0.1152s/iter; left time: 789.4460s\n",
            "\titers: 500, epoch: 4 | loss: 0.0051786\n",
            "\tspeed: 0.1150s/iter; left time: 776.3980s\n",
            "\titers: 600, epoch: 4 | loss: 0.0077562\n",
            "\tspeed: 0.1150s/iter; left time: 765.1651s\n",
            "\titers: 700, epoch: 4 | loss: 0.0066862\n",
            "\tspeed: 0.1148s/iter; left time: 752.3801s\n",
            "\titers: 800, epoch: 4 | loss: 0.0056582\n",
            "\tspeed: 0.1151s/iter; left time: 742.7566s\n",
            "\titers: 900, epoch: 4 | loss: 0.0053529\n",
            "\tspeed: 0.1152s/iter; left time: 731.6030s\n",
            "\titers: 1000, epoch: 4 | loss: 0.0061164\n",
            "\tspeed: 0.1152s/iter; left time: 720.4871s\n",
            "Epoch: 4 cost time: 119.71073508262634\n",
            "Epoch: 4, Steps: 1036 | Train Loss: 0.0063210 Vali Loss: 0.0237439 Test Loss: 0.0217680\n",
            "Validation loss decreased (0.023898 --> 0.023744).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.0044023\n",
            "\tspeed: 0.4609s/iter; left time: 2819.1427s\n",
            "\titers: 200, epoch: 5 | loss: 0.0044992\n",
            "\tspeed: 0.1154s/iter; left time: 694.2940s\n",
            "\titers: 300, epoch: 5 | loss: 0.0051983\n",
            "\tspeed: 0.1153s/iter; left time: 682.2358s\n",
            "\titers: 400, epoch: 5 | loss: 0.0027728\n",
            "\tspeed: 0.1156s/iter; left time: 672.6045s\n",
            "\titers: 500, epoch: 5 | loss: 0.0037037\n",
            "\tspeed: 0.1154s/iter; left time: 659.4915s\n",
            "\titers: 600, epoch: 5 | loss: 0.0045525\n",
            "\tspeed: 0.1151s/iter; left time: 646.6995s\n",
            "\titers: 700, epoch: 5 | loss: 0.0052811\n",
            "\tspeed: 0.1152s/iter; left time: 635.4238s\n",
            "\titers: 800, epoch: 5 | loss: 0.0045321\n",
            "\tspeed: 0.1153s/iter; left time: 624.7499s\n",
            "\titers: 900, epoch: 5 | loss: 0.0076483\n",
            "\tspeed: 0.1153s/iter; left time: 612.8233s\n",
            "\titers: 1000, epoch: 5 | loss: 0.0079427\n",
            "\tspeed: 0.1155s/iter; left time: 602.5212s\n",
            "Epoch: 5 cost time: 119.82832503318787\n",
            "Epoch: 5, Steps: 1036 | Train Loss: 0.0060153 Vali Loss: 0.0238072 Test Loss: 0.0227011\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.0054700\n",
            "\tspeed: 0.4578s/iter; left time: 2326.0001s\n",
            "\titers: 200, epoch: 6 | loss: 0.0068611\n",
            "\tspeed: 0.1154s/iter; left time: 574.6603s\n",
            "\titers: 300, epoch: 6 | loss: 0.0054642\n",
            "\tspeed: 0.1155s/iter; left time: 563.9653s\n",
            "\titers: 400, epoch: 6 | loss: 0.0056121\n",
            "\tspeed: 0.1153s/iter; left time: 551.4497s\n",
            "\titers: 500, epoch: 6 | loss: 0.0067668\n",
            "\tspeed: 0.1151s/iter; left time: 538.5513s\n",
            "\titers: 600, epoch: 6 | loss: 0.0055619\n",
            "\tspeed: 0.1150s/iter; left time: 526.8331s\n",
            "\titers: 700, epoch: 6 | loss: 0.0079453\n",
            "\tspeed: 0.1151s/iter; left time: 515.7347s\n",
            "\titers: 800, epoch: 6 | loss: 0.0069426\n",
            "\tspeed: 0.1150s/iter; left time: 503.6117s\n",
            "\titers: 900, epoch: 6 | loss: 0.0033952\n",
            "\tspeed: 0.1153s/iter; left time: 493.5891s\n",
            "\titers: 1000, epoch: 6 | loss: 0.0075477\n",
            "\tspeed: 0.1151s/iter; left time: 481.1718s\n",
            "Epoch: 6 cost time: 119.73680448532104\n",
            "Epoch: 6, Steps: 1036 | Train Loss: 0.0058149 Vali Loss: 0.0234661 Test Loss: 0.0229049\n",
            "Validation loss decreased (0.023744 --> 0.023466).  Saving model ...\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 0.0046499\n",
            "\tspeed: 0.4616s/iter; left time: 1867.2134s\n",
            "\titers: 200, epoch: 7 | loss: 0.0055838\n",
            "\tspeed: 0.1156s/iter; left time: 456.0504s\n",
            "\titers: 300, epoch: 7 | loss: 0.0040560\n",
            "\tspeed: 0.1157s/iter; left time: 444.7333s\n",
            "\titers: 400, epoch: 7 | loss: 0.0073734\n",
            "\tspeed: 0.1154s/iter; left time: 432.3277s\n",
            "\titers: 500, epoch: 7 | loss: 0.0046113\n",
            "\tspeed: 0.1151s/iter; left time: 419.7212s\n",
            "\titers: 600, epoch: 7 | loss: 0.0046999\n",
            "\tspeed: 0.1152s/iter; left time: 408.5160s\n",
            "\titers: 700, epoch: 7 | loss: 0.0047175\n",
            "\tspeed: 0.1149s/iter; left time: 395.8560s\n",
            "\titers: 800, epoch: 7 | loss: 0.0046210\n",
            "\tspeed: 0.1150s/iter; left time: 384.6563s\n",
            "\titers: 900, epoch: 7 | loss: 0.0025269\n",
            "\tspeed: 0.1150s/iter; left time: 373.2861s\n",
            "\titers: 1000, epoch: 7 | loss: 0.0079568\n",
            "\tspeed: 0.1152s/iter; left time: 362.4060s\n",
            "Epoch: 7 cost time: 119.78524923324585\n",
            "Epoch: 7, Steps: 1036 | Train Loss: 0.0056794 Vali Loss: 0.0238698 Test Loss: 0.0226214\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 1.5625e-06\n",
            "\titers: 100, epoch: 8 | loss: 0.0053128\n",
            "\tspeed: 0.4584s/iter; left time: 1379.2422s\n",
            "\titers: 200, epoch: 8 | loss: 0.0031663\n",
            "\tspeed: 0.1154s/iter; left time: 335.8087s\n",
            "\titers: 300, epoch: 8 | loss: 0.0018734\n",
            "\tspeed: 0.1152s/iter; left time: 323.5170s\n",
            "\titers: 400, epoch: 8 | loss: 0.0067281\n",
            "\tspeed: 0.1155s/iter; left time: 312.9639s\n",
            "\titers: 500, epoch: 8 | loss: 0.0070713\n",
            "\tspeed: 0.1152s/iter; left time: 300.6337s\n",
            "\titers: 600, epoch: 8 | loss: 0.0040456\n",
            "\tspeed: 0.1150s/iter; left time: 288.5592s\n",
            "\titers: 700, epoch: 8 | loss: 0.0049638\n",
            "\tspeed: 0.1154s/iter; left time: 277.9034s\n",
            "\titers: 800, epoch: 8 | loss: 0.0029226\n",
            "\tspeed: 0.1151s/iter; left time: 265.7755s\n",
            "\titers: 900, epoch: 8 | loss: 0.0035575\n",
            "\tspeed: 0.1151s/iter; left time: 254.2132s\n",
            "\titers: 1000, epoch: 8 | loss: 0.0078542\n",
            "\tspeed: 0.1157s/iter; left time: 243.9333s\n",
            "Epoch: 8 cost time: 119.79093098640442\n",
            "Epoch: 8, Steps: 1036 | Train Loss: 0.0055896 Vali Loss: 0.0236416 Test Loss: 0.0229166\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 7.8125e-07\n",
            "\titers: 100, epoch: 9 | loss: 0.0053224\n",
            "\tspeed: 0.4596s/iter; left time: 906.8053s\n",
            "\titers: 200, epoch: 9 | loss: 0.0041815\n",
            "\tspeed: 0.1156s/iter; left time: 216.5488s\n",
            "\titers: 300, epoch: 9 | loss: 0.0063658\n",
            "\tspeed: 0.1155s/iter; left time: 204.7221s\n",
            "\titers: 400, epoch: 9 | loss: 0.0039647\n",
            "\tspeed: 0.1155s/iter; left time: 193.2141s\n",
            "\titers: 500, epoch: 9 | loss: 0.0085397\n",
            "\tspeed: 0.1148s/iter; left time: 180.6354s\n",
            "\titers: 600, epoch: 9 | loss: 0.0075585\n",
            "\tspeed: 0.1149s/iter; left time: 169.3083s\n",
            "\titers: 700, epoch: 9 | loss: 0.0068615\n",
            "\tspeed: 0.1151s/iter; left time: 158.0627s\n",
            "\titers: 800, epoch: 9 | loss: 0.0037116\n",
            "\tspeed: 0.1151s/iter; left time: 146.4647s\n",
            "\titers: 900, epoch: 9 | loss: 0.0097056\n",
            "\tspeed: 0.1151s/iter; left time: 135.0632s\n",
            "\titers: 1000, epoch: 9 | loss: 0.0024175\n",
            "\tspeed: 0.1153s/iter; left time: 123.6990s\n",
            "Epoch: 9 cost time: 119.73689031600952\n",
            "Epoch: 9, Steps: 1036 | Train Loss: 0.0055377 Vali Loss: 0.0236224 Test Loss: 0.0228462\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : q64_Autoformer_custom_ftS_sl24_ll1_pl72_dm512_nh32_el6_dl5_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 9429\n",
            "test shape: (9429, 72, 1) (9429, 72, 1)\n",
            "test shape: (9429, 72, 1) (9429, 72, 1)\n",
            "mse:0.022923404350876808, mae:0.10909528285264969, r2:0.9695428013801575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olPTN8Wx8dDR",
        "outputId": "179cd2f1-c35b-41da-d4cd-6749b710ba08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'target': 'q64', 'des': 'Exp', 'dropout': 0.05, 'num_workers': 10, 'gpu': 0, 'lradj': 'type1', 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False, 'freq': 'h', 'checkpoints': './checkpoints/', 'bucket_size': 4, 'n_hashes': 4, 'seq_len': 24, 'label_len': 1, 'pred_len': 2, 'e_layers': 6, 'd_layers': 5, 'n_heads': 32, 'factor': 1, 'd_model': 512, 'itr': 1, 'd_ff': 2048, 'moving_avg': 25, 'distil': True, 'output_attention': False, 'patience': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'embed': 'timeF', 'activation': 'gelu', 'use_amp': False, 'loss': 'mse', 'train_epochs': 10, 'is_training': True, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'root_path': './dataset/', 'data_path': 'mucnuoc_gio_preprocess.csv', 'model_id': 'q64', 'model': 'Autoformer', 'data': 'custom', 'features': 'S'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "save_model_simple(exp, args)\n",
        "print(\"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "lXHmtI6_g_gh",
        "outputId": "324727ef-5a50-4fed-9173-b656cba60ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ’¾ Saving model to: /content/save_model\n",
            "âœ… Model saved: autoformer_model_window_q64.h5\n",
            "Debugging scaler save...\n",
            "   Trying method 2: from data_loader directly...\n",
            "train 33157\n",
            "   Found scaler in attribute: scaler\n",
            "âœ… Scaler saved: scaler_q64_20250614_053727.pkl\n",
            "72\n",
            "âœ… Config saved: config_autoformer_window_q64.pkl\n",
            "ðŸ“ All files saved in: /content/save_model\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}